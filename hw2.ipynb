{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3 [10 баллов] \n",
    "# До 30.04.18 23:59\n",
    "\n",
    "Задание выполняется в группе (1-4 человека). В случае использования какого-либо строннего источника информации обязательно дайте на него ссылку (поскольку другие тоже могут на него наткнуться). Плагиат наказывается нулём баллов за задание и предвзятым отношением в будущем.\n",
    "\n",
    "Не все части обязательны для выполнения, однако вы можете быть дополнительно оштрафованы за небрежное за выполнение одной или двух частей вместо четырех.\n",
    "\n",
    "При возниконовении проблем с выполнением задания обращайтесь с вопросами к преподавателю. Поэтому настоятельно рекомендуется выполнять задание заранее, оставив запас времени на всевозможные технические проблемы. Если вы начали читать условие в последний вечер и не успели из-за проблем с установкой какой-либо библиотеки — это ваши проблемы.\n",
    "\n",
    "\n",
    "Результат выполнения задания — это отчёт в формате html на основе Jupyter Notebook. Нормальный отчёт должен включать в себя:\n",
    "* Краткую постановку задачи и формулировку задания\n",
    "* Описание **минимума** необходимой теории и/или описание используемых инструментов - не стоит переписывать лекции или Википедию\n",
    "* Подробный пошаговый рассказ о проделанной работе\n",
    "* Аккуратно оформленные результаты\n",
    "* **Внятные выводы** – не стоит относится к домашнему заданию как к последовательности сугубо технических шагов, а стоит относится скорее как к небольшому практическому исследованию, у которого есть своя цель и свое назначение.\n",
    "\n",
    "Небрежное его оформление отчета существенно отразится на итоговой оценке. Весь код из отчёта должен быть воспроизводимым, если для этого нужны какие-то дополнительные действия, установленные модули и т.п. — всё это должно быть прописано в тексте в явном виде.\n",
    "\n",
    "Сдача отчетов осуществляется через систему AnyTask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование архитектуры SENNA для определения части речи\n",
    "\n",
    "Домашнее задание написано по мотивам работы R. Collobert:\n",
    "\n",
    "**Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. \"Natural language processing (almost) from scratch.\" Journal of Machine Learning Research 12, no. Aug (2011): 2493-2537.**\n",
    "\n",
    "В этом домашнем задании вам предстоит самостоятельно разработать архитектуру SENNA для определения части речи. \n",
    "SENNA – это простая архитектура нейронной сети, позволяющая достигнуть state-of-the-art результатов в нескольких задачах обработки текстов.  \n",
    "\n",
    "Использование SENNA для определения части речи предполагает, что задача определения части речи для данного слова формулируется как задача классификации: пусть в размеченном корпусе всего $|T|$ (= tagset) различных тегов частей речи, тогда каждое слово $w$ относится к одному из $T$ классов. Для каждого слова из обучающих данных формируется собственный вектор признаков. Нейронная сеть обучается по всем векторам признаков для слов из обучающего множества. \n",
    "\n",
    "Подход к решению задачи классификации представлен в оригинальной статье на рис. 1 (Figure 1: Window approach network). Он состоит из следующих шагов (раздел 3.3.1):\n",
    "1. Каждое слово представляется эмбеддингом: $w_i \\rightarrow LT_{w^i}$, размерность эмбеддинга - $d$;\n",
    "2. Для каждого слова формируется окно длины $k$ из $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова, $k$ – нечетное. \n",
    "3. Для каждого слова формируется вектор признаков, состоящий из конкатенированных эмбеддингов слов из левого окна, данного слова и слов из правого окна. Итоговая размерность вектора признаков – $d \\times k$. Именно этот вектор подается на вход нейронной сети;\n",
    "4. Обучается нейронная сеть, имеющая один скрытый слой с $n_h$ нейроннами и нелинейной функцией активации $\\theta$;\n",
    "5. На выходном слое нейронной сети решается задача классификации на |T| классов, то есть, определяется часть речи для каждого слова. \n",
    "\n",
    "Если для слова невозможно найти $(k-1)/2$ соседних слов слева от данного слова  и $(k-1)/2$ соседних слов справа от данного слова – используется padding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "1. Открытый корпус: https://github.com/dialogue-evaluation/morphoRuEval-2017/blob/master/OpenCorpora_Texts.rar\n",
    "2. Предобученные эмбеддинги Facebook: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ru.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1 [2 балла] Подготовка данных\n",
    "1. Прочитайте размеченные данные Открытого корпуса, используя nltk.corpus.reader.conll.ConllCorpusReader\n",
    "2. Посчитайте количество предложений и число тегов частей речи;\n",
    "3. Сформируйте тестовое и обучающее множество: первые 3/4 данных – обучающее множество;\n",
    "\n",
    "Для каждого слова:\n",
    "1. Определите его окно (слова слева и справа) размера $k$;\n",
    "2. Сформируйте его вектор признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = ConllCorpusReader('.', ['unamb_sent_14_6.conllu'], ('ignore', 'words', 'ignore', 'pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38508\n"
     ]
    }
   ],
   "source": [
    "sents = reader.tagged_sents()\n",
    "N = len(sents)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число тегов частей речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "all_pos = list(set(list(map(lambda x: x[1], reader.tagged_words()))))\n",
    "NUM_CLASSES = len(all_pos)\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим предложения на обучающую и тестирующую часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = sents[:-N // 4]\n",
    "TEST = sents[-N // 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функуции чтения эмбеддингов и подготовки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embeddings(max_words = -1):\n",
    "    words = []\n",
    "    embeddings = []\n",
    "    file = open('wiki.ru.vec', 'r', encoding='UTF-8')\n",
    "    file.readline()\n",
    "    for i, line in enumerate(file):\n",
    "        if max_words != -1 and i >= max_words:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        split_result = line.rsplit(maxsplit=300)\n",
    "        word = split_result[0]\n",
    "        embedding = np.array(split_result[1:],dtype=float)\n",
    "        words.append(word)\n",
    "        embeddings.append(embedding)\n",
    "    return words, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(sents, word_to_idx, embeddings, pos_idx, k):\n",
    "    p = (k - 1) // 2\n",
    "    features = []\n",
    "    labels = []\n",
    "    for sent in sents:\n",
    "        u = 0\n",
    "        sent_embeddings = []\n",
    "        unknown_embedding = np.zeros(300, dtype=float)\n",
    "        sent_embeddings.extend([unknown_embedding] * p)\n",
    "        for word, tag in sent:\n",
    "            word = word.lower()\n",
    "            if word in word_to_idx:\n",
    "                word_embedding = embeddings[word_to_idx[word]]\n",
    "            else:\n",
    "                word_embedding = unknown_embedding\n",
    "            sent_embeddings.append(word_embedding)\n",
    "            labels.append(pos_idx[tag])\n",
    "            u += 1\n",
    "        sent_embeddings.extend([unknown_embedding] * p)\n",
    "        for i in range(u):\n",
    "            current = np.array(sent_embeddings[i:i + k])\n",
    "            features.append(current.flatten())\n",
    "    return np.array(features, dtype=float), np.array(labels,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_to_idx(words):\n",
    "    res = {}\n",
    "    for i, word in enumerate(words):\n",
    "        res[word] = i\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем эмбеддинги (ограничиваемся 50000 самых популярных слов) и формируем датасет c окном ширины 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words, embeddings = read_embeddings(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx = build_word_to_idx(words)\n",
    "pos_idx = build_word_to_idx(all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "X_train, Y_train = prepare_dataset(TRAIN, word_to_idx, embeddings, pos_idx, k)\n",
    "X_test, Y_test = prepare_dataset(TEST, word_to_idx, embeddings, pos_idx, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2 [4 баллов] Архитектура нейронной сети\n",
    "\n",
    "Архитектура нейронной сети состоит из следующих слов:\n",
    "1. Входной слой: нейронная сеть получает на вход вектор признаков, состоящий из $k$ конкатенированных эмбеддингов;/\n",
    "2. Скрытый слой: $n_h$ нейронов и нелинейная функция активации $\\theta$;\n",
    "3. Выходной слой:  $|T|$ нейронов для итоговой классификации.\n",
    "\n",
    "Обучите нейронную сеть на обучающих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(n_h, k, act):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_h, activation=act, input_shape=(k * 300,)))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model(128, k, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 14s 44us/step - loss: 0.3842 - acc: 0.8710 - val_loss: 0.4123 - val_acc: 0.8567\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 12s 38us/step - loss: 0.2871 - acc: 0.8995 - val_loss: 0.3877 - val_acc: 0.8666\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 12s 38us/step - loss: 0.2590 - acc: 0.9087 - val_loss: 0.3851 - val_acc: 0.8674\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 12s 38us/step - loss: 0.2410 - acc: 0.9144 - val_loss: 0.3789 - val_acc: 0.8705\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 12s 38us/step - loss: 0.2272 - acc: 0.9189 - val_loss: 0.3928 - val_acc: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a9acc4f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=64, epochs=1000, validation_split=0.1, \\\n",
    "          callbacks=[keras.callbacks.EarlyStopping('val_acc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3 [1 балл] Оценка качества\n",
    "\n",
    "Протестируйте нейронную сеть на тестовых данных. Используйте accuracy для оценки качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107228/107228 [==============================] - 4s 34us/step\n",
      "0.890439064428\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили качество порядка 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4 [1 балл] Оптимизация гиперпарметров\n",
    "\n",
    "В эксперименте участвуют следующие гиперпараметры:\n",
    "* $k$ – размер окна;\n",
    "* $n_h$ – число нейронов на скрытом слое;\n",
    "* $\\theta$ – вид функции активации.\n",
    "\n",
    "Оцените их влияние на качество модели. Как увеличение окна или числа нейронов влияет на итоговый показатель качества? Зависит ли итоговый показатель качества от функции активации на скрытом слое? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_measure_quality(X_train, Y_train, X_test, Y_test, k, n_h, thetha):\n",
    "    print('Building model...')\n",
    "    model = build_model(n_h, k, thetha)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit(X_train, Y_train, batch_size=64, epochs=1000, validation_split=0.1, \\\n",
    "          callbacks=[keras.callbacks.EarlyStopping('val_acc')])\n",
    "    _, accuracy = model.evaluate(X_test, Y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fit_and_measure_quality(k, n_h, thetha):\n",
    "    print('Loading train dataset...')\n",
    "    X_train, Y_train = prepare_dataset(TRAIN, word_to_idx, embeddings, pos_idx, k)\n",
    "    print('Loading test dataset...')\n",
    "    X_test, Y_test = prepare_dataset(TEST, word_to_idx, embeddings, pos_idx, k)\n",
    "    return fit_and_measure_quality(X_train, Y_train, X_test, Y_test, k, n_h, thetha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим влияние ширины окна на показатель качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for k=1\n",
      "Loading train dataset...\n",
      "Loading test dataset...\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 12s 37us/step - loss: 0.4810 - acc: 0.8283 - val_loss: 0.5028 - val_acc: 0.8149\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 11s 36us/step - loss: 0.4084 - acc: 0.8462 - val_loss: 0.4845 - val_acc: 0.8190\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 11s 36us/step - loss: 0.3980 - acc: 0.8484 - val_loss: 0.4793 - val_acc: 0.8201\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 11s 36us/step - loss: 0.3928 - acc: 0.8495 - val_loss: 0.4799 - val_acc: 0.8226\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 11s 36us/step - loss: 0.3897 - acc: 0.8501 - val_loss: 0.4714 - val_acc: 0.8219\n",
      "107228/107228 [==============================] - 3s 28us/step\n",
      "k=1: Accuracy=0.841329\n",
      "Calculating for k=3\n",
      "Loading train dataset...\n",
      "Loading test dataset...\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 12s 39us/step - loss: 0.3841 - acc: 0.8714 - val_loss: 0.4051 - val_acc: 0.8599\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 12s 39us/step - loss: 0.2862 - acc: 0.9001 - val_loss: 0.3832 - val_acc: 0.8647\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 12s 39us/step - loss: 0.2585 - acc: 0.9089 - val_loss: 0.3770 - val_acc: 0.8705\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 13s 40us/step - loss: 0.2406 - acc: 0.9148 - val_loss: 0.3824 - val_acc: 0.8694\n",
      "107228/107228 [==============================] - 4s 33us/step\n",
      "k=3: Accuracy=0.890150\n",
      "Calculating for k=5\n",
      "Loading train dataset...\n",
      "Loading test dataset...\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 15s 47us/step - loss: 0.3838 - acc: 0.8728 - val_loss: 0.3960 - val_acc: 0.8645\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 14s 44us/step - loss: 0.2751 - acc: 0.9055 - val_loss: 0.3810 - val_acc: 0.8700\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 14s 44us/step - loss: 0.2383 - acc: 0.9174 - val_loss: 0.3787 - val_acc: 0.8717\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 13s 43us/step - loss: 0.2116 - acc: 0.9257 - val_loss: 0.3825 - val_acc: 0.8736\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 13s 42us/step - loss: 0.1894 - acc: 0.9333 - val_loss: 0.4057 - val_acc: 0.8718\n",
      "107228/107228 [==============================] - 4s 40us/step\n",
      "k=5: Accuracy=0.894794\n",
      "Calculating for k=7\n",
      "Loading train dataset...\n",
      "Loading test dataset...\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 24s 75us/step - loss: 0.3900 - acc: 0.8712 - val_loss: 0.4100 - val_acc: 0.8639\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 23s 73us/step - loss: 0.2694 - acc: 0.9075 - val_loss: 0.3895 - val_acc: 0.8690\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 23s 72us/step - loss: 0.2269 - acc: 0.9204 - val_loss: 0.3970 - val_acc: 0.8692\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 23s 73us/step - loss: 0.1950 - acc: 0.9319 - val_loss: 0.4042 - val_acc: 0.8706\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 23s 72us/step - loss: 0.1685 - acc: 0.9407 - val_loss: 0.4278 - val_acc: 0.8715\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 23s 72us/step - loss: 0.1455 - acc: 0.9485 - val_loss: 0.4477 - val_acc: 0.8714\n",
      "107228/107228 [==============================] - 8s 75us/step\n",
      "k=7: Accuracy=0.891950\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 7]:\n",
    "    print(\"Calculating for k=%d\" % k)\n",
    "    quality = load_fit_and_measure_quality(k, 128, 'relu')\n",
    "    print(\"k=%d: Accuracy=%f\" % (k, quality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что использование окна шириной 3 заметно лучше чем единичное окно, однако дальнейшее увеличение не приводит к сильному росту качества. Тем не менее есть некоторый прирост при k=5, дальше качество не растет, так что используем k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем оптимальное значение нейронов скрытого слоя. Обычно для подобного параметра должен быть некоторый минимум, т.к. при небольшом количестве наблюдается недообучение, а при большом напротив - переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "X_train, Y_train = prepare_dataset(TRAIN, word_to_idx, embeddings, pos_idx, k)\n",
    "X_test, Y_test = prepare_dataset(TEST, word_to_idx, embeddings, pos_idx, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for n_h=32\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 22s 69us/step - loss: 0.4302 - acc: 0.8608 - val_loss: 0.4276 - val_acc: 0.8538\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.3033 - acc: 0.8959 - val_loss: 0.4096 - val_acc: 0.8613\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2797 - acc: 0.9034 - val_loss: 0.3916 - val_acc: 0.8669\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2643 - acc: 0.9089 - val_loss: 0.3866 - val_acc: 0.8679\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2520 - acc: 0.9124 - val_loss: 0.3907 - val_acc: 0.8686\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 22s 69us/step - loss: 0.2429 - acc: 0.9147 - val_loss: 0.3948 - val_acc: 0.8684\n",
      "107228/107228 [==============================] - 7s 69us/step\n",
      "n_h=32: Accuracy=0.891931\n",
      "Calculating for n_h=64\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 22s 69us/step - loss: 0.4034 - acc: 0.8682 - val_loss: 0.4148 - val_acc: 0.8592\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 21s 68us/step - loss: 0.2887 - acc: 0.9004 - val_loss: 0.3942 - val_acc: 0.8647\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 21s 68us/step - loss: 0.2586 - acc: 0.9103 - val_loss: 0.3918 - val_acc: 0.8703\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 21s 68us/step - loss: 0.2364 - acc: 0.9177 - val_loss: 0.3904 - val_acc: 0.8711\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2200 - acc: 0.9230 - val_loss: 0.3967 - val_acc: 0.8712\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 21s 68us/step - loss: 0.2062 - acc: 0.9274 - val_loss: 0.4023 - val_acc: 0.8719\n",
      "Epoch 7/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.1950 - acc: 0.9314 - val_loss: 0.4234 - val_acc: 0.8714\n",
      "107228/107228 [==============================] - 7s 69us/step\n",
      "n_h=64: Accuracy=0.892453\n",
      "Calculating for n_h=128\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.3841 - acc: 0.8728 - val_loss: 0.4116 - val_acc: 0.8579\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 21s 66us/step - loss: 0.2746 - acc: 0.9051 - val_loss: 0.3835 - val_acc: 0.8710\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.2382 - acc: 0.9167 - val_loss: 0.3816 - val_acc: 0.8720\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.2115 - acc: 0.9257 - val_loss: 0.3915 - val_acc: 0.8723\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.1903 - acc: 0.9325 - val_loss: 0.4032 - val_acc: 0.8714\n",
      "107228/107228 [==============================] - 7s 68us/step\n",
      "n_h=128: Accuracy=0.894337\n",
      "Calculating for n_h=256\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.3713 - acc: 0.8756 - val_loss: 0.3990 - val_acc: 0.8650\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2614 - acc: 0.9099 - val_loss: 0.3844 - val_acc: 0.8692\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 21s 67us/step - loss: 0.2200 - acc: 0.9229 - val_loss: 0.3895 - val_acc: 0.8736\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 21s 66us/step - loss: 0.1884 - acc: 0.9334 - val_loss: 0.4046 - val_acc: 0.8724\n",
      "107228/107228 [==============================] - 7s 69us/step\n",
      "n_h=256: Accuracy=0.893787\n",
      "Calculating for n_h=512\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 23s 72us/step - loss: 0.3613 - acc: 0.8784 - val_loss: 0.4013 - val_acc: 0.8630\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 22s 71us/step - loss: 0.2536 - acc: 0.9120 - val_loss: 0.3835 - val_acc: 0.8724\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 22s 70us/step - loss: 0.2078 - acc: 0.9271 - val_loss: 0.3907 - val_acc: 0.8731\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 22s 71us/step - loss: 0.1715 - acc: 0.9395 - val_loss: 0.4222 - val_acc: 0.8738\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 23s 72us/step - loss: 0.1436 - acc: 0.9498 - val_loss: 0.4593 - val_acc: 0.8732\n",
      "107228/107228 [==============================] - 8s 75us/step\n",
      "n_h=512: Accuracy=0.893386\n"
     ]
    }
   ],
   "source": [
    "for n_h in [32, 64, 128, 256, 512]:\n",
    "    print(\"Calculating for n_h=%d\" % n_h)\n",
    "    quality = fit_and_measure_quality(X_train, Y_train, X_test, Y_test, k, n_h, 'relu')\n",
    "    print(\"n_h=%d: Accuracy=%f\" % (n_h, quality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по результатам оптимальное число нейронов должно лежать между 128 и 256, переберем некоторые значения в этом диапазоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for n_h=120\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 66us/step - loss: 0.3854 - acc: 0.8721 - val_loss: 0.4129 - val_acc: 0.8596\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.2756 - acc: 0.9048 - val_loss: 0.3911 - val_acc: 0.8671\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.2394 - acc: 0.9161 - val_loss: 0.3844 - val_acc: 0.8703\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2134 - acc: 0.9252 - val_loss: 0.3931 - val_acc: 0.8721\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1920 - acc: 0.9322 - val_loss: 0.4070 - val_acc: 0.8724\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1747 - acc: 0.9379 - val_loss: 0.4182 - val_acc: 0.8737\n",
      "Epoch 7/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1595 - acc: 0.9430 - val_loss: 0.4446 - val_acc: 0.8691\n",
      "107228/107228 [==============================] - 7s 68us/step\n",
      "n_h=120: Accuracy=0.893890\n",
      "Calculating for n_h=160\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.3807 - acc: 0.8732 - val_loss: 0.4075 - val_acc: 0.8602\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2698 - acc: 0.9071 - val_loss: 0.3884 - val_acc: 0.8671\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2317 - acc: 0.9190 - val_loss: 0.3916 - val_acc: 0.8700\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2032 - acc: 0.9290 - val_loss: 0.3938 - val_acc: 0.8728\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.1801 - acc: 0.9367 - val_loss: 0.4148 - val_acc: 0.8724\n",
      "107228/107228 [==============================] - 7s 66us/step\n",
      "n_h=160: Accuracy=0.895484\n",
      "Calculating for n_h=200\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 20s 65us/step - loss: 0.3754 - acc: 0.8746 - val_loss: 0.4101 - val_acc: 0.8608\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2661 - acc: 0.9079 - val_loss: 0.3748 - val_acc: 0.8715\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2264 - acc: 0.9207 - val_loss: 0.3843 - val_acc: 0.8722\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1969 - acc: 0.9310 - val_loss: 0.3970 - val_acc: 0.8752\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1717 - acc: 0.9392 - val_loss: 0.4188 - val_acc: 0.8717\n",
      "107228/107228 [==============================] - 7s 66us/step\n",
      "n_h=200: Accuracy=0.891773\n",
      "Calculating for n_h=240\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 66us/step - loss: 0.3729 - acc: 0.8752 - val_loss: 0.3981 - val_acc: 0.8635\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2618 - acc: 0.9095 - val_loss: 0.3739 - val_acc: 0.8718\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2216 - acc: 0.9226 - val_loss: 0.3764 - val_acc: 0.8759\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.1912 - acc: 0.9323 - val_loss: 0.4038 - val_acc: 0.8737\n",
      "107228/107228 [==============================] - 7s 65us/step\n",
      "n_h=240: Accuracy=0.895214\n"
     ]
    }
   ],
   "source": [
    "for n_h in [120, 160, 200, 240]:\n",
    "    print(\"Calculating for n_h=%d\" % n_h)\n",
    "    quality = fit_and_measure_quality(X_train, Y_train, X_test, Y_test, k, n_h, 'relu')\n",
    "    print(\"n_h=%d: Accuracy=%f\" % (n_h, quality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остановимся на значении с максимальным accuracy: 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем функцию активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for relu\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 20s 65us/step - loss: 0.3784 - acc: 0.8738 - val_loss: 0.3987 - val_acc: 0.8639\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2694 - acc: 0.9068 - val_loss: 0.3799 - val_acc: 0.8717\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2313 - acc: 0.9191 - val_loss: 0.3843 - val_acc: 0.8721\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2032 - acc: 0.9288 - val_loss: 0.4060 - val_acc: 0.8722\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1802 - acc: 0.9367 - val_loss: 0.4128 - val_acc: 0.8732\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.1599 - acc: 0.9437 - val_loss: 0.4285 - val_acc: 0.8731\n",
      "107228/107228 [==============================] - 7s 66us/step\n",
      "function=relu: Accuracy=0.894300\n",
      "Calculating for elu\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.3855 - acc: 0.8709 - val_loss: 0.4125 - val_acc: 0.8579\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.2882 - acc: 0.9004 - val_loss: 0.3884 - val_acc: 0.8693\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2541 - acc: 0.9118 - val_loss: 0.3863 - val_acc: 0.8694\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2290 - acc: 0.9197 - val_loss: 0.3836 - val_acc: 0.8725\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.2072 - acc: 0.9270 - val_loss: 0.3919 - val_acc: 0.8737\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 19s 62us/step - loss: 0.1883 - acc: 0.9329 - val_loss: 0.4043 - val_acc: 0.8738\n",
      "Epoch 7/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.1714 - acc: 0.9390 - val_loss: 0.4203 - val_acc: 0.8718\n",
      "107228/107228 [==============================] - 7s 66us/step\n",
      "function=elu: Accuracy=0.893796\n",
      "Calculating for selu\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 66us/step - loss: 0.3904 - acc: 0.8695 - val_loss: 0.4199 - val_acc: 0.8571\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.3040 - acc: 0.8961 - val_loss: 0.4037 - val_acc: 0.8633\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2740 - acc: 0.9058 - val_loss: 0.3991 - val_acc: 0.8648\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2514 - acc: 0.9131 - val_loss: 0.3952 - val_acc: 0.8680\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2330 - acc: 0.9189 - val_loss: 0.3858 - val_acc: 0.8723\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2179 - acc: 0.9234 - val_loss: 0.3928 - val_acc: 0.8725\n",
      "Epoch 7/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2050 - acc: 0.9281 - val_loss: 0.4071 - val_acc: 0.8725\n",
      "107228/107228 [==============================] - 8s 72us/step\n",
      "function=selu: Accuracy=0.893386\n",
      "Calculating for sigmoid\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.4402 - acc: 0.8594 - val_loss: 0.4201 - val_acc: 0.8559\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2972 - acc: 0.8982 - val_loss: 0.3922 - val_acc: 0.8652\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.2686 - acc: 0.9074 - val_loss: 0.3921 - val_acc: 0.8672\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.2466 - acc: 0.9148 - val_loss: 0.3718 - val_acc: 0.8720\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.2287 - acc: 0.9205 - val_loss: 0.3701 - val_acc: 0.8764\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 19s 61us/step - loss: 0.2119 - acc: 0.9261 - val_loss: 0.3768 - val_acc: 0.8751\n",
      "107228/107228 [==============================] - 8s 70us/step\n",
      "function=sigmoid: Accuracy=0.896893\n",
      "Calculating for tanh\n",
      "Building model...\n",
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 21s 65us/step - loss: 0.3846 - acc: 0.8719 - val_loss: 0.4129 - val_acc: 0.8581\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2848 - acc: 0.9018 - val_loss: 0.3977 - val_acc: 0.8655\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2500 - acc: 0.9132 - val_loss: 0.3906 - val_acc: 0.8710\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2233 - acc: 0.9222 - val_loss: 0.3875 - val_acc: 0.8723\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 64us/step - loss: 0.2000 - acc: 0.9301 - val_loss: 0.3929 - val_acc: 0.8732\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.1807 - acc: 0.9371 - val_loss: 0.3993 - val_acc: 0.8736\n",
      "Epoch 7/1000\n",
      "315319/315319 [==============================] - 20s 62us/step - loss: 0.1628 - acc: 0.9439 - val_loss: 0.4150 - val_acc: 0.8722\n",
      "107228/107228 [==============================] - 8s 71us/step\n",
      "function=tanh: Accuracy=0.894971\n"
     ]
    }
   ],
   "source": [
    "n_h = 160\n",
    "for nonlinearity in ['relu', 'elu', 'selu', 'sigmoid', 'tanh']:\n",
    "    print(\"Calculating for %s\" % nonlinearity)\n",
    "    quality = fit_and_measure_quality(X_train, Y_train, X_test, Y_test, k, n_h, nonlinearity)\n",
    "    print(\"function=%s: Accuracy=%f\" % (nonlinearity, quality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что выбор функции активации практически не влияет на результат. Остановимся на той, которая дает лучший результат - sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговые подобранные значения гиперпараметров\n",
    "    - k=5\n",
    "    - 160 неронов на скрытом слое\n",
    "    - Нелинейность - sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5 [2 балла] Анализ ошибок\n",
    "1. Привидите примеры из тестового множества, на которых нейронная сеть ошибается. Объясните, почему возникают ошибки.\n",
    "2. Протестируйте нейронную сеть на произвольном предложении (не из тестовых данных). Возникают ли ошибки? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель с оптимальными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 315319 samples, validate on 35036 samples\n",
      "Epoch 1/1000\n",
      "315319/315319 [==============================] - 20s 65us/step - loss: 0.4352 - acc: 0.8603 - val_loss: 0.4183 - val_acc: 0.8577\n",
      "Epoch 2/1000\n",
      "315319/315319 [==============================] - 19s 62us/step - loss: 0.2963 - acc: 0.8989 - val_loss: 0.4002 - val_acc: 0.8628\n",
      "Epoch 3/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2678 - acc: 0.9072 - val_loss: 0.3870 - val_acc: 0.8697\n",
      "Epoch 4/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2460 - acc: 0.9146 - val_loss: 0.3768 - val_acc: 0.8731\n",
      "Epoch 5/1000\n",
      "315319/315319 [==============================] - 20s 63us/step - loss: 0.2280 - acc: 0.9212 - val_loss: 0.3760 - val_acc: 0.8739\n",
      "Epoch 6/1000\n",
      "315319/315319 [==============================] - 19s 62us/step - loss: 0.2116 - acc: 0.9261 - val_loss: 0.3774 - val_acc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b5f5228d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(n_h, k, 'sigmoid')\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=64, epochs=1000, validation_split=0.1, \\\n",
    "      callbacks=[keras.callbacks.EarlyStopping('val_acc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция тестирования модели на размеченном предложении: возвращает долю ошибок в этом предложении и список строк с описаниями ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_on_tagged_sentence(model, sent):\n",
    "    features, labels = prepare_dataset([sent], word_to_idx, embeddings, pos_idx, k)\n",
    "    predicted_labels = model.predict_classes(features)\n",
    "    sent_predictions = list(map( lambda x: all_pos[x], predicted_labels))\n",
    "    sent_accuracy = sum(predicted_labels == labels) / len(labels)\n",
    "    errors = []\n",
    "    for tagged, prediction in zip(sent, sent_predictions):\n",
    "        word, actual = tagged\n",
    "        if actual != prediction:\n",
    "            errors.append(\"Word %s: Expected %s Prediction %s\" % (word, actual, prediction))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sent(tagged_sent):\n",
    "    words = []\n",
    "    for word, _ in tagged_sent:\n",
    "        words.append(word)\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отберем достаточно длинные предложения (не менее 7 слов), на которых модель допускает не менее 4 ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for sent in TEST:\n",
    "    if len(sent) < 7:\n",
    "        continue\n",
    "    errors = test_model_on_tagged_sentence(model, sent)\n",
    "    if len(errors) >= 4:\n",
    "        results.append((sent, errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты 3 таких предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кажется сомнительным , что те « приёмы » в работе , которые студенты используют для обхода ошибок , наличествующих в используемом СГА проприетарном ПО , соответствуют миссии СГА — предоставлению образования высшего качества .\n",
      "Word используемом: Expected ADJ Prediction NOUN\n",
      "Word СГА: Expected PROPN Prediction NOUN\n",
      "Word проприетарном: Expected ADJ Prediction VERB\n",
      "Word ПО: Expected NOUN Prediction ADP\n",
      "Word СГА: Expected PROPN Prediction X\n",
      "Word предоставлению: Expected NOUN Prediction ADJ\n",
      "\n",
      "Из-за мелких размеров постройки / проходы гнезда незаметны , обитают в естественных полостях почвы , фураж осуществляют в почве и листовом опаде .\n",
      "Word Из-за: Expected ADP Prediction NUM\n",
      "Word незаметны: Expected ADJ Prediction NOUN\n",
      "Word полостях: Expected NOUN Prediction ADJ\n",
      "Word фураж: Expected NOUN Prediction ADJ\n",
      "Word опаде: Expected X Prediction NOUN\n",
      "\n",
      "Традиция признаёт Цзо чжуань комментарием к летописи Чунь цю , однако трактовка и подробности событий не совпадают с летописью и другими комментариями .\n",
      "Word Цзо: Expected X Prediction PUNCT\n",
      "Word чжуань: Expected X Prediction DET\n",
      "Word комментарием: Expected NOUN Prediction ADJ\n",
      "Word цю: Expected X Prediction PROPN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "shuffle(results)\n",
    "for sent, errors in results[:3]:\n",
    "    print_sent(sent)\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех 3 предложениях ошибки возникают в основном на словах, которых нет в 50000 самых популярных слов из таблицы эмбеддингов - иногда из-за редкости самого слова, а иногда из-за редкости конкретной словоформы, а информации из левого и правого контекста недостаточно для однозначного определения части речи. Исключением является разве что слово \"комментарием\" из 3-его предложения, в нем судя по всему ошибка связана с тем, что в левом контексте оба слова с неизвестным эмбеддингом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем модель на произвольном предложении не из выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
