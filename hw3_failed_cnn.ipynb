{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов с активным обучением\n",
    "\n",
    "\n",
    "Зададимся простой задачей классификации текстов: например, классификацией отзывов на банки по тональности. Эта задача решается с достаточно высокими показателями качества с использованием стандартных алгоритмов классификации, например, сверточных нейронных сетей: корпус состоит из достаточного количества документов, чтобы сверточная сеть хорошо обучилась. Однако возникает естественный вопрос: действительно ли все документы нужны для того, чтобы достичь таких высоких показателей качества (или сопоставимых с ними). Парадигма активного обучения поможет вам ответить на этот вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предобработка данных [2 балла]\n",
    "\n",
    "Коллекция отзывов хранится в файле banki_responses (https://www.dropbox.com/s/ol3ux3ibr6rd5ke/banki_responses.json.bz2?dl=0). Одна строчка в этом файле соответствует одному json-словарю. Из этого словаря вам понадобятся два значения по ключам text и rating -- текст отзыва и его оценка по шкале от 1 до 5.   \n",
    "\n",
    "Считайте файл. Посчитайте, каких отзывов больше: положительных или отрицательных? \n",
    "\n",
    "Проведите предварительную обработку данных: удалите слишком короткие и слишком длинные тексты (пороги на длину определите самостоятельно). \n",
    "\n",
    "Разбейте данные на обучающее ($train$) и тестовое ($test$) множество случайным образом в отношеннии 3:1 (или любом другом отношении, которое покажется вам разумным). \n",
    "\n",
    "Задача классификации сформулирована так: по каждому отзыву определить его оценку (т.е. классификация на 5 классов). Признаками для классификации выступают слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136189\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nib4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a62d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "texts = []\n",
    "grades = []\n",
    "with open('./banki_responses.json', 'r') as f:\n",
    "    for line in f:\n",
    "        parsed = json.loads(line.strip())\n",
    "        if parsed['rating_grade'] is not None: # delete responses without grade\n",
    "            texts.append(parsed['text'])\n",
    "            grades.append(parsed['rating_grade'])\n",
    "texts = np.array(texts)\n",
    "grades = np.array(grades)\n",
    "print(len(texts))\n",
    "plt.hist(grades, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92235 32252\n"
     ]
    }
   ],
   "source": [
    "print(len(grades[grades < 3]), len(grades[grades > 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Видно, что негативные отзывы преобладают почти в три раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a3af28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(map(len, texts)), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> отсечем тексты, длины которых находятся после 90-перцентили и до 10-перцентили, тем самым оставив 80% выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472.0 2913.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats import mquantiles\n",
    "texts_lens = np.array(list(map(len, texts)))\n",
    "low, high = mquantiles(texts_lens, [0.1, 0.9])\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109017"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (texts_lens >= low) & (texts_lens <= high)\n",
    "texts = texts[mask]\n",
    "grades = grades[mask]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Baseline [4 балла]\n",
    "\n",
    "Получите baseline классификации: в идеале, используйте сверточную нейронную сеть (слой эмбеддингов + свертка + субдескритизация). Число и размерность фильтров определите самостоятельно, так же как и использование регуляризаторов (dropout / batch norm) и их параметров. Так же самостоятельно (но обосновано) решите, использовать ли вам предобученные эмбеддинги или нет и проводить ли вам лемматизацию или нет. \n",
    "\n",
    "Обучите сеть на обучающем множестве и протестируйте на тестовом. Зафиксируйте baseline.\n",
    "\n",
    "Если совсем трудно или вычисления занимают слишком много времени, используйте любой другой известный и симпатичный вам алгоритм классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaning\n",
    "import stop_words\n",
    "ru_stop_words = stop_words.get_stop_words('russian')\n",
    "import re\n",
    "rewords = re.compile('[а-яА-ЯёЁ]+')\n",
    "clean = lambda word : word not in ru_stop_words\n",
    "lower = lambda word : word.lower()\n",
    "\n",
    "words = np.array(list(map(lambda text: list(filter(clean, map(lower, rewords.findall(text)))), texts)))\n",
    "words_len = np.array(list(map(len, words)))\n",
    "\n",
    "mask = words_len > 0\n",
    "words = words[mask]\n",
    "grades = grades[mask]\n",
    "MAX_WORDS = words_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "embeddings_model = fastText.load_model('./wiki.ru.bin')\n",
    "DIM = embeddings_model.get_dimension()\n",
    "\n",
    "def get_embeddings(words):\n",
    "    return np.array(list(map(lambda word : embeddings_model.get_word_vector(word), words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109015,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = np.array(list(map(get_embeddings, words)))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkhismatullin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 64\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 3, activation='relu', input_shape=(MAX_WORDS, DIM)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    return model\n",
    "baseline_model = build_model()\n",
    "baseline_model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, grades-1, test_size=test_size, shuffle=True, stratify=grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_zeros_to_batch(embdeggings, targets, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for i, e in enumerate(embdeggings):\n",
    "        batch_x.append(np.append(e, np.zeros((MAX_WORDS - e.shape[0]) * DIM).reshape(-1, DIM), axis=0))\n",
    "        batch_y.append(targets[i])\n",
    "        if len(batch_x) == batch_size:\n",
    "            yield np.array(batch_x), np.array(batch_y)\n",
    "            batch_x.clear()\n",
    "            batch_y.clear()\n",
    "#     if len(batch_x) > 0:\n",
    "#         yield np.array(batch_x), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1277/1277 [==============================] - 299s 234ms/step - loss: 0.9523 - acc: 0.6639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17389ee10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit_generator(add_zeros_to_batch(X_train, y_train, BATCH_SIZE), \n",
    "                             steps_per_epoch= len(X_train) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6863703705646373\n"
     ]
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 1000\n",
    "_, accuracy = baseline_model.evaluate_generator(add_zeros_to_batch(X_test, y_test, TEST_BATCH_SIZE), \n",
    "                                                steps= len(X_test) // TEST_BATCH_SIZE)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline: 0.6863 accuracy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Baseline [4 балла] Активное обучение\n",
    "\n",
    "Подход активного обучения основан на следующей идее: вместо всего обучающего множества мы используем его маленькие фрагменты, в которых модель неуверена для обучения. Таким образом, модель обучается исключительно по **трудным** объектам, число которых существенно меньше, чем общее число объектов.\n",
    "\n",
    "Обучение модели начинается с обучения по $N$ случайно выбранным примерам, где $N$ – небольшое число (100, 200 и т.д.). Затем модель тестируется на $|train| - N$ объектах, после чего из  $|train| - N$ объектов выбираются снова $N$  объектов, в которых модель не уверена. Эти объекты используются для дообучения модели. Процесс выбора $N$ трудных объектов и дообучения на них повторяется некоторое количество раз (100, 200 и т.д. раз). На каждом шаге активного обучения модель можно протестировать на тестовом множестве, чтобы сравнить ее качества с baseline.\n",
    "\n",
    "Как выбирать трудные объекты:\n",
    "1. Выход нейронной сети - оценки 5 вероятностей принадлежности объекта одному из классов. Предсказанный класс – это тот класс, вероятность которого максимальна. Отсортируем объекты по убыванию вероятности предсказанного класса ($\\min \\max p_i$) и выберем $N$ первых объектов;\n",
    "2. Используем энтропию: чем больше энтропия предсказания, тем ближе распределение вероятностей предсказания к равномерному распределению, тем труднее объект. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AL_BATCH_SIZE = 256\n",
    "KERAS_BATCH_SIZE = 32\n",
    "N_STEPS = 100\n",
    "active_learning_model = build_model()\n",
    "active_learning_model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 1.3179 - acc: 0.5664\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 1.5021 - acc: 0.3359\n",
      "Step 1: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 1.4725 - acc: 0.2383\n",
      "Step 2: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 1.4323 - acc: 0.2227\n",
      "Step 3: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 1.3766 - acc: 0.4375\n",
      "Step 4: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 1.4676 - acc: 0.2734\n",
      "Step 5: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 1.4324 - acc: 0.1836\n",
      "Step 6: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 1.3835 - acc: 0.4766\n",
      "Step 7: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 1.4504 - acc: 0.3047\n",
      "Step 8: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 1.5426 - acc: 0.1992\n",
      "Step 9: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 1.5002 - acc: 0.2227\n",
      "Step 10: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 113ms/step - loss: 1.4523 - acc: 0.3320\n",
      "Step 11: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 1.3945 - acc: 0.3750\n",
      "Step 12: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 1.4049 - acc: 0.4766\n",
      "Step 13: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 1.4359 - acc: 0.3828\n",
      "Step 14: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.3499 - acc: 0.3828\n",
      "Step 15: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 134ms/step - loss: 1.4215 - acc: 0.3086\n",
      "Step 16: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 1.4849 - acc: 0.3828\n",
      "Step 17: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 106ms/step - loss: 1.4418 - acc: 0.2734\n",
      "Step 18: accuracy on test 0.5407\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 1.4327 - acc: 0.3125\n",
      "Step 19: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 1.4909 - acc: 0.2734\n",
      "Step 20: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 1.4659 - acc: 0.4375\n",
      "Step 21: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 1.4910 - acc: 0.3555\n",
      "Step 22: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 1.5642 - acc: 0.2812\n",
      "Step 23: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 92ms/step - loss: 1.5889 - acc: 0.2539\n",
      "Step 24: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.2954 - acc: 0.5469\n",
      "Step 25: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 1.2833 - acc: 0.4883\n",
      "Step 26: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 95ms/step - loss: 1.2301 - acc: 0.3125\n",
      "Step 27: accuracy on test 0.1883\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 1.3999 - acc: 0.2891\n",
      "Step 28: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.2813 - acc: 0.3008\n",
      "Step 29: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 1.3089 - acc: 0.5000\n",
      "Step 30: accuracy on test 0.5341\n",
      "Epoch 1/1\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 1.2430 - acc: 0.6211\n",
      "Step 31: accuracy on test 0.5341\n"
     ]
    }
   ],
   "source": [
    "remaining_samples = np.arange(len(X_train))\n",
    "chosen = np.random.choice(remaining_samples, AL_BATCH_SIZE, replace=False)\n",
    "remaining_samples = remaining_samples[~np.isin(remaining_samples, chosen)]\n",
    "\n",
    "\n",
    "active_learning_model.fit_generator(add_zeros_to_batch(X_train[chosen], y_train[chosen], KERAS_BATCH_SIZE), \n",
    "                                 steps_per_epoch= AL_BATCH_SIZE // KERAS_BATCH_SIZE)\n",
    "\n",
    "accuracies = [active_learning_model.evaluate_generator(add_zeros_to_batch(X_test, y_test, TEST_BATCH_SIZE),\n",
    "                                                steps= len(X_test) // TEST_BATCH_SIZE,\n",
    "                                                workers=8)[1]]\n",
    "for step in range(N_STEPS):\n",
    "    # predict on train\n",
    "    predicted = active_learning_model.predict_generator(\n",
    "        add_zeros_to_batch(X_train[remaining_samples], y_train[remaining_samples], KERAS_BATCH_SIZE), \n",
    "        steps= len(remaining_samples) // KERAS_BATCH_SIZE,\n",
    "        workers=8)\n",
    "    \n",
    "    # get max predicted probability on each case\n",
    "    predicted_max = predicted.max(axis=1)\n",
    "    # get batch with the least max probability\n",
    "    batch_mask = remaining_samples[predicted_max.argsort()[:AL_BATCH_SIZE]]\n",
    "    # fit model\n",
    "    active_learning_model.fit_generator(add_zeros_to_batch(X_train[batch_mask], y_train[batch_mask], KERAS_BATCH_SIZE), \n",
    "                                 steps_per_epoch= AL_BATCH_SIZE // KERAS_BATCH_SIZE)\n",
    "    # delete trat samples from train\n",
    "    remaining_samples = remaining_samples[~np.isin(remaining_samples, batch_mask)]\n",
    "    \n",
    "    acc = active_learning_model.evaluate_generator(add_zeros_to_batch(X_test, y_test, TEST_BATCH_SIZE), \n",
    "                                                    steps=len(X_test) // TEST_BATCH_SIZE, \n",
    "                                                   workers=8)[1]\n",
    "    accuracies.append(acc)\n",
    "    print('Step %d: accuracy on test %.4f' % (step + 1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# См hw3_fully_connected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
